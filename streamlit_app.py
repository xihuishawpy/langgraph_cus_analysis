"""Simple Streamlit front-end for the Pro Search agent."""
from __future__ import annotations

import os
import sys
from pathlib import Path
from typing import Any, Dict, List

import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage

PROJECT_ROOT = Path(__file__).resolve().parent
BACKEND_SRC = PROJECT_ROOT / "backend" / "src"
if str(BACKEND_SRC) not in sys.path:
    sys.path.insert(0, str(BACKEND_SRC))

from agent.graph import graph  # noqa: E402

st.set_page_config(page_title="Pro Search Agent è°ƒè¯•å°", layout="wide")
st.title("ğŸ” Pro Search Agent è°ƒè¯•å°")

REQUIRED_KEYS = ["DASHSCOPE_API_KEY", "TAVILY_API_KEY"]
missing_keys = [name for name in REQUIRED_KEYS if not os.getenv(name)]
if missing_keys:
    st.error(
        "ç¼ºå°‘ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨ç»ˆç«¯æˆ– .env ä¸­é…ç½®åå†è¿è¡Œï¼š\n" + ", ".join(missing_keys)
    )
    st.stop()

with st.sidebar:
    st.header("è¿è¡Œé…ç½®")
    initial_queries = st.number_input("åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡", min_value=1, max_value=5, value=3)
    max_loops = st.slider("æœ€å¤§ç ”ç©¶å¾ªç¯", min_value=1, max_value=5, value=2)
    use_kb_search = st.checkbox("å¯ç”¨å†…éƒ¨çŸ¥è¯†åº“æ£€ç´¢", value=False)
    kb_top_k = st.slider(
        "çŸ¥è¯†åº“è¿”å›æ¡æ•°",
        min_value=1,
        max_value=10,
        value=3,
        disabled=not use_kb_search,
    )
    query_model = st.text_input("æŸ¥è¯¢ç”Ÿæˆæ¨¡å‹", value="qwen-plus")
    reflection_model = st.text_input("åæ€æ¨¡å‹", value="qwen-plus")
    answer_model = st.text_input("å›ç­”æ¨¡å‹", value="qwen-plus")
    reasoning_model = st.text_input("æ¨ç†æ¨¡å‹ (å¯é€‰)", value="")
    embedding_model = st.text_input(
        "çŸ¥è¯†åº“å‘é‡æ¨¡å‹", value="text-embedding-v3", disabled=not use_kb_search
    )
    llm_backend = st.selectbox("LLM åç«¯", options=["dashscope", "local"], index=0)
    enable_tongyi_search_summary = st.checkbox(
        "ä½¿ç”¨é€šä¹‰åƒé—®ç”Ÿæˆæœç´¢æ‘˜è¦", value=False, help="é»˜è®¤å…³é—­ï¼Œå¯åœ¨éœ€è¦æ›´è¯¦ç»†ç»¼è¿°æ—¶å¼€å¯"
    )

if "runs" not in st.session_state:
    st.session_state["runs"] = []

st.write("è¾“å…¥è°ƒç ”é—®é¢˜ï¼Œç‚¹å‡»â€œå¼€å§‹è°ƒç ”â€å³å¯æŸ¥çœ‹å®Œæ•´é“¾è·¯è¾“å‡ºã€‚")
user_query = st.text_area("ç ”ç©¶é—®é¢˜", height=120, placeholder="ä¾‹å¦‚ï¼šPCB å¢é•¿è¾ƒå¥½çš„ä¼ä¸šåˆ†æ")
run_button = st.button("å¼€å§‹è°ƒç ”", type="primary", disabled=not user_query.strip())

configurable_overrides: Dict[str, Any] = {
    "number_of_initial_queries": int(initial_queries),
    "max_research_loops": int(max_loops),
    "knowledge_base_top_k": int(kb_top_k),
    "query_generator_model": query_model.strip() or "qwen-plus",
    "reflection_model": reflection_model.strip() or "qwen-plus",
    "answer_model": answer_model.strip() or "qwen-plus",
    "knowledge_base_paths": os.getenv(
        "KNOWLEDGE_BASE_PATHS",
        "eastmoney_concept_constituents.xlsx,sw_third_industry_constituents.xlsx",
    ),
    "knowledge_base_embedding_model": embedding_model.strip() or "text-embedding-v3",
    "llm_backend": llm_backend,
    "enable_knowledge_base_search": bool(use_kb_search),
    "enable_tongyi_search_summary": bool(enable_tongyi_search_summary),
}

if run_button:
    st.session_state["runs"].insert(0, {"query": user_query.strip(), "status": "running"})
    try:
        with st.spinner("æ™ºèƒ½ä½“æ­£åœ¨æ‰§è¡Œ..."):
            state: Dict[str, Any] = {
                "messages": [HumanMessage(content=user_query.strip())],
            }
            if reasoning_model.strip():
                state["reasoning_model"] = reasoning_model.strip()
            result = graph.invoke(state, config={"configurable": configurable_overrides})
    except Exception as exc:  # noqa: BLE001
        st.session_state["runs"][0] = {
            "query": user_query.strip(),
            "status": "error",
            "error": str(exc),
        }
    else:
        st.session_state["runs"][0] = {
            "query": user_query.strip(),
            "status": "success",
            "result": result,
        }

for idx, run in enumerate(st.session_state["runs"], start=1):
    with st.expander(f"è¿è¡Œ {idx}: {run['query'][:40]}" + ("..." if len(run["query"]) > 40 else ""), expanded=(idx == 1)):
        if run["status"] == "error":
            st.error(run.get("error", "æœªçŸ¥é”™è¯¯"))
            continue
        if run["status"] == "running":
            st.info("ä»»åŠ¡æ‰§è¡Œä¸­...")
            continue
        result = run["result"]
        messages: List[Any] = result.get("messages", [])
        answer = None
        for message in reversed(messages):
            if isinstance(message, AIMessage):
                answer = message.content
                break
        if answer:
            st.subheader("æœ€ç»ˆå›ç­”")
            st.markdown(answer)
        summaries = result.get("web_research_result", [])
        if summaries:
            st.subheader("é˜¶æ®µæ€§æ‘˜è¦")
            for i, summary in enumerate(summaries, start=1):
                st.markdown(f"**æ‘˜è¦ {i}:**\n{summary}")
        sources = result.get("sources_gathered", [])
        if sources:
            st.subheader("å¼•ç”¨æ¥æº")
            for source in sources:
                label = source.get("label") or source.get("short_url")
                url = source.get("value") or source.get("short_url")
                st.markdown(f"- [{label}]({url})")
        st.caption("åŸå§‹çŠ¶æ€: " + repr({k: v for k, v in result.items() if k != "messages"}))
